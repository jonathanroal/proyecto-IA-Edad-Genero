{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Proyecto.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sL2Y_doisaxE"},"source":["#IMPORTS\n","from google.colab import drive  #dar acceso a mi drive\n","import os                       #funciones del sistema operativo\n","import cv2                      #libreria para procesamiento de imagenes\n","from google.colab.patches import cv2_imshow #metodo de open cv para monstar imagenes, lo usaremos para verificar informacion\n","import numpy as np              #libreria para trabjar con arreglos y estructuras con muchas dimenciones\n","import matplotlib.pyplot as plt #libreria para hacer plots de los datos \n","\n","import pandas as pd\n","\n","#sklearn => una de las mejores librerias de machine learning en python\n","from sklearn.metrics import confusion_matrix      #nos permite evaluar el desempeño del algoritmo, nos muestra las clases y cantidad con la cantidad predicha por el modelo, nos muestra si el algoritmos confunde clases\n","from sklearn.metrics import classification_report #nos permite ver la calidad de las predicciones realizadas por el modelo\n","\n","import seaborn as sns           #libreria de visualizacion de datos\n","\n","#imports para el modelado\n","from tensorflow.keras.layers import Dropout                     #\"marginacion\" capa que ayuda a evitar el overfiting, aleatoriamente escoje valores y los cambia a 0 en intervalos\n","from tensorflow.keras.layers import Flatten,BatchNormalization  #Aplanar(flatten) es la función que convierte el mapa de características agrupado en una sola columna que se pasa a la capa completamente conectada.\n","                                                                #La normalización por lotes(BatchNormalization) es una técnica diseñada para estandarizar automáticamente las entradas a una capa en una red neuronal de aprendizaje profundo.\n","from tensorflow.keras.layers import Dense, MaxPooling2D,Conv2D  #es una capa de convolución 2D, esta capa crea un núcleo de convolución que es entrada de capas que ayuda a producir un tensor de salida\n","from tensorflow.keras.layers import Input,Activation,Add\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQ90P5Raqb2t"},"source":["#Importo mi unidad de drive dando acceso para obtener los dataset y guardar contenido\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"khl7VmddsVr9"},"source":["#direccion donde esta mi proyecto\n","fldr = \"drive/MyDrive/Colab Notebooks/Proyecto IA/\"\n","files=  os.listdir(fldr+'/DataClean')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwzawKQ5s4tn"},"source":["#NO CORRER (solo si hay nuevo dataset), LA INFORMACION YA FUE TRANSFORMADA--------------------------------------------------------------------\n","#Dura casi 2 horas tratando los datos en colab\n","#Creamos los arreglos que contendran la informacion tomada del data set, cada uno se correspondera por los indices\n","ages    = []\n","genders = []\n","images  = []\n","\n","#Estructura de los datos, cada imagen viene etiquetada con su respectiva edad y genero\n","#28_1_0_20170117122007867.jpg.chip.jpg\n","\n","#Extraemos la informacion del dataset para prepararla para el modelo\n","for photo in files:\n","  age=int(photo.split('_')[0])\n","  gender=int(photo.split('_')[1])\n","  img_dir=fldr+'/DataClean/'+photo #url de la imagen\n"," \n","  image=cv2.imread(img_dir)\n","\n","  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #simplificamos los tonos de color de las fotos para realzar la imagen\n","  image= cv2.resize(image,(48,48))               #estandarizamos el tamanno de la imagen pasando todas las imagenes en un formato de 48X48 pixceles esto generara un total de 2304 entradas para el modelo\n","  images.append(image)\n","  ages.append(age)\n","  genders.append(gender)\n","  #NO CORRER, LA INFORMACION YA FUE TRANSFORMADA--------------------------------------------------------------------"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4Mo58dltY89"},"source":["#Comprobamos que los tamannos de los arreglos obtenidos del data set se correspondan mutuamente\n","print(\"TODOS deben ser del mismo largo: \",\" Edades \" ,len(ages), \" Generos \",len(genders), \" Fotos \"  ,len(images))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EmlCL7C4to9_"},"source":["#Elegimos una imagen aleatoria para corroborar que la imagen corresponda al genero y edad correspondiente aproximada\n","cv2_imshow(images[24])\n","print(\"Edad: \", ages[24], \" Genero: \", genders[24])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CftsttIvubPa"},"source":["#Se transforman en arrays de pandas para guardar los datasets\n","images_=np.array(images)\n","genders_=np.array(genders)\n","ages_=np.array(ages)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2W098DUDvHcj"},"source":["#guardo la infromacion para no tener tratarla nuevamente en futuras ejecuciones\n","#np.save(fldr+'/SalidasProyectoIA/image.npy',images_)\n","#np.save(fldr+'/SalidasProyectoIA/gender.npy',genders_)\n","#np.save(fldr+'/SalidasProyectoIA/age.npy',ages_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"osU8j9WdEA9L"},"source":["#NO USAR SI NO ES NECESARIO\n","#CON ESTA FUNCION CARGO LA DATA YA MANIPULADA-----------------------------------------------------\n","images_ = np.load(\"drive/MyDrive/Colab Notebooks/Proyecto IA/SalidasProyectoIA/image.npy\")\n","genders_ = np.load(\"drive/MyDrive/Colab Notebooks/Proyecto IA/SalidasProyectoIA/gender.npy\")\n","ages_  = np.load(\"drive/MyDrive/Colab Notebooks/Proyecto IA/SalidasProyectoIA/age.npy\")\n","#CON ESTA FUNCION CARGO LA DATA YA MANIPULADA-----------------------------------------------------"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ApaexKGhRhzL"},"source":["#Comprobamos que todos siguen teniendo el mismo tamanno\n","print(\"TODOS deben ser del mismo largo: \",\" Edades \" ,len(ages_), \" Generos \",len(genders_), \" Fotos \"  ,len(images_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-34h2uu9vQwD"},"source":["#Me aseguro que solo existan dos generos \n","values, counts = np.unique(genders_, return_counts=True)\n","print(counts ,\" \", values )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUZguHggwANT"},"source":["#realizo un plot de los generos para ver la distribucion del data set\n","fig = plt.figure()\n","ax = fig.add_axes([0,0,1,1])\n","gender = ['Male', 'Female']\n","values=[counts[0],counts[1]] \n","ax.bar(gender,values)\n","plt.xlabel('Genero')\n","plt.ylabel('Cantidad')\n","#fig.savefig(fldr+'/SalidasProyectoIA/images/generosPreEliminacion.png')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BwYmt7EwQQz"},"source":["#Visualizo las edades que estan presentes en la data\n","values, counts = np.unique(ages_, return_counts=True)\n","print(counts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbReWMq7fmcG"},"source":["#Vemos claramente una distribucion mal realizada en las edades\n","plt.hist(ages_, 50, facecolor='green', alpha=0.75)\n","plt.xlabel('Edad')\n","plt.ylabel('Cantidad')\n","#plt.savefig(fldr+'/SalidasProyectoIA/images/edadesPreEliminacion.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkXguGVhmE3k"},"source":["'''#NO USAR, PERJUDICA EL MODELO-------------------------------------------------------------\n","#Se procede con la eliminacion de la informacion de los menores a 1 año\n","\n","index_array = np.where(ages_ <= 4)[0] #creamos un nuevo array solo con la los indices de los elementos de edad de 1 año\n","#print(new_array) #indices de donde estan los elementos asociados a 1 anno\n","\n","\n","ages_2    = np.delete(ages_, index_array, axis=0) \n","genders_2 = np.delete(genders_f, index_array, axis=0) \n","images_2  = np.delete( images_, index_array,  axis=0)  \n","#NO USAR, PERJUDICA EL MODELO-------------------------------------------------------------'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XyR6rEDMSVUj"},"source":["cv2_imshow(images_[2999])\n","print(genders_[2999] ,  \" \", ages_[2999])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ag4t-YQYSCAf"},"source":["#index_array = np.where(ages_ <= 4)[0]\n","#min(index_array)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V76ll-wipZvb"},"source":["#Comprobamos que todos siguen teniendo el mismo tamanno\n","print(\"TODOS deben ser del mismo largo: \",\" Edades \" ,len(ages_2), \" Generos \",len(genders_2), \" Fotos \"  ,len(images_2))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3kQHY_pkphCh"},"source":["cv2_imshow(images_2[2999])\n","print(genders_2[2999] ,  \" \", ages_2[2999])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L8SD9zHpobla"},"source":["#NUEVA DISTRIBUCION DE EDADES\n","#Aqui podemos observar una mejor distribucion, adicionalmente eliminamos los individuos de 1 año, los cuales presentan \n","#caracteristicas androgenas y afectarian igualmente la prediccion de genero\n","plt.hist(ages_2, 50, facecolor='green', alpha=0.75)\n","plt.xlabel('Edad')\n","plt.ylabel('Cantidad')\n","#plt.savefig(fldr+'/SalidasProyectoIA/images/edadesPostEliminacion.png') #guardo una imagen del plot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PESY9N6wlxe"},"source":["#NUEVA DISTRIBUCION DE GENEROS\n","#Observamos la nuevas distribucion de los generos, vemos que continuan muy similares\n","\n","values, counts = np.unique(genders_2, return_counts=True)\n","\n","fig = plt.figure()\n","ax = fig.add_axes([0,0,1,1])\n","gender = ['Male', 'Female']\n","values=[counts[0],counts[1]] \n","ax.bar(gender,values)\n","plt.xlabel('Genero')\n","plt.ylabel('Cantidad')\n","#fig.savefig(fldr+'/SalidasProyectoIA/images/generosPostEliminacion.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4c0J4LH1v2H"},"source":["#Reemplazamos el valor de las variables anteriores\n","images_  = images_2\n","genders_ = genders_2\n","ages_    = ages_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9_hTN3K0bOd4"},"source":["ages_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oMiR3uk2Pwz"},"source":["#Comprobamos que todos siguen teniendo el mismo tamanno\n","print(\"TODOS deben ser del mismo largo: \",\" Edades \" ,len(ages_), \" Generos \",len(genders_), \" Fotos \"  ,len(images_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VGpRuitj2bXv"},"source":["#frecuencia de personas que tienen determinada edad\n","values, counts = np.unique(ages_, return_counts=True)\n","print(counts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XM0dUQ822U_x"},"source":["#Otra manera de visualizar edades\n","val=values.tolist()\n","cnt=counts.tolist()\n","plt.plot(counts)\n","plt.xlabel('ages')\n","plt.ylabel('distribution')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkADA5BG4A4h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-KG4Yjt3I8q"},"source":["#Creamos las etiquetas que usaremos mas adelante para el modelo\n","labels=[]\n","\n","i=0\n","#uniremos las edades y generos en un solo array como etiquetas\n","while i<len(ages_):\n","  label=[]\n","  label.append([ages_[i]])\n","  label.append([genders_[i]])\n","  labels.append(label)\n","  i+=1\n","#labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ot6sBbjN3nEE"},"source":["images_2=images_/255   #estandarizo el valor del color de los pixeles cuyo maximo valor es 255, aqui lo pongo entre un 1 y 0 cosa que debeficia al modelado\n","labels_f=np.array(labels) #le doy forma a las etiquetas de array de numpy\n","images_2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbdP0VfM6YNB"},"source":["images_2[1] #Podemos apreciar los pixeles estandarizados, ahora ocupan valores entre 0 y 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8RcvKohc6vGf"},"source":["labels_f=np.array(labels)\n","labels_f.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WxYeqtKo7adF"},"source":["#Separamos los datas entre entrenamiento y testeo con la siguiente funcion, usando 75% entrenamiento, 25% testeo\n","X_train, X_test, Y_train, Y_test= train_test_split(images_2, labels_f,test_size=0.25)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4PWKcSE8ebn"},"source":["#vemos las estiquetas de entrenamiento\n","Y_train[0:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kn3-SVd48nyq"},"source":["#vemos la data de entrenamiento\n","X_train[0:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SflxFT187UV"},"source":["#Creamos un arreglo para entrenamiento y testeo combinando las dos features ( : significa tomar todo)\n","Y_train_2=[Y_train[:,1],Y_train[:,0]]\n","Y_test_2=[Y_test[:,1],Y_test[:,0]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yig_GzI9Lc3"},"source":["Y_train_2[0][0:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFjYhWA89sLJ"},"source":["Y_train_2[1][0:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mu9jNjqg95uj"},"source":["#MODELO\n","\n","def Convolution(input_tensor,filters):\n","    \n","    x = Conv2D(filters=filters,kernel_size=(3, 3),padding = 'same',strides=(1, 1),kernel_regularizer=l2(0.001))(input_tensor)\n","    x = Dropout(0.1)(x)\n","    x= Activation('relu')(x)\n","\n","    return x\n","def model(input_shape):\n","  inputs = Input((input_shape))\n","  \n","  conv_1= Convolution(inputs,32)\n","  maxp_1 = MaxPooling2D(pool_size = (2,2)) (conv_1)\n","  conv_2 = Convolution(maxp_1,64)\n","  maxp_2 = MaxPooling2D(pool_size = (2, 2)) (conv_2)\n","  conv_3 = Convolution(maxp_2,128)\n","  maxp_3 = MaxPooling2D(pool_size = (2, 2)) (conv_3)\n","  conv_4 = Convolution(maxp_3,256)\n","  maxp_4 = MaxPooling2D(pool_size = (2, 2)) (conv_4)\n","  flatten= Flatten() (maxp_4)\n","  dense_1= Dense(64,activation='relu')(flatten)\n","  dense_2= Dense(64,activation='relu')(flatten)\n","  drop_1=Dropout(0.2)(dense_1)\n","  drop_2=Dropout(0.2)(dense_2)\n","  output_1= Dense(1,activation=\"sigmoid\",name='sex_out')(drop_1)\n","  output_2= Dense(1,activation=\"relu\",name='age_out')(drop_2)\n","  model = Model(inputs=[inputs], outputs=[output_1,output_2])\n","  model.compile(loss=[\"binary_crossentropy\",\"mae\"], optimizer=\"Adam\",\n","\tmetrics=[\"accuracy\"])\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ah5svfj-GAe"},"source":["Model=model((48,48,3))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvrOtegr-Pno"},"source":["Model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IA9OrhK5-RsD"},"source":["\n","fle_s='sex_age_model.h5'\n","checkpointer = ModelCheckpoint(fle_s, monitor='val_loss',verbose=1,save_best_only=True,save_weights_only=False, mode='auto',save_freq='epoch')\n","Early_stop=tf.keras.callbacks.EarlyStopping(patience=75, monitor='val_loss',restore_best_weights=True),\n","callback_list=[checkpointer,Early_stop]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eyosf7r_-aM3"},"source":["History=Model.fit(X_train,Y_train_2,batch_size=64,validation_data=(X_test,Y_test_2),epochs=500,callbacks=[callback_list])\n","#Pare el modelo anticipadamente"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNa77pEs_-y9"},"source":["Model.evaluate(X_test,Y_test_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Enz2z-kPk9KA"},"source":["# convierto el historico en un dataframe de pandas   \n","hist_df = pd.DataFrame(History.history) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cp6XP7wXlMWK"},"source":["#convierto el historico en csv y lo guardo\n","hist_df.to_csv(fldr+'/modelosGenerados/modelo2')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKSuj8wWErU4"},"source":["#Cargo el historico guardado como csv\n","history = pd.read_csv(fldr+'/SalidasProyectoIA/history')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JI3KZEe7AIyD"},"source":["pred=Model.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0U9OzllALE3"},"source":["pred[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fzwbAb6uAO7-"},"source":["#hago un plot para ver el comportamiento del modelo\n","plt.plot(History.history['loss'])\n","plt.plot(History.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n","                        wspace=0.35)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVvQN-bBAR_O"},"source":["plt.plot(History.history['sex_out_accuracy'])\n","plt.plot(History.history['age_out_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n","                        wspace=0.35)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FRf5_BQgYdCJ"},"source":["fig, ax = plt.subplots()\n","ax.scatter(Y_test_2[1], pred[1])\n","ax.plot([Y_test_2[1].min(),Y_test_2[1].max()], [Y_test_2[1].min(), Y_test_2[1].max()], 'k--', lw=4)\n","ax.set_xlabel('Actual Age')\n","ax.set_ylabel('Predicted Age')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5S-h2xXYd2x"},"source":["#guardo el modelo\n","# Guardar el Modelo\n","Model.save('drive/MyDrive/Colab Notebooks/Proyecto IA/modelosGenerados/Model2.h5')\n","# Recrea exactamente el mismo modelo solo desde el archivo\n","#new_model = keras.models.load_model('path_to_my_model.h5')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZiHFkkLYgQh"},"source":["i=0\n","Pred_l=[]\n","while(i<len(pred[0])):\n","\n","  Pred_l.append(int(np.round(pred[0][i])))\n","  i+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pSRVSmQIYnKR"},"source":["report=classification_report(Y_test_2[0], Pred_l)\n","print(report)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijvPrVIpYwwc"},"source":["results = confusion_matrix(Y_test_2[0], Pred_l)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"alnhzL4xY2DH"},"source":["sns.heatmap(results, annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFjrhW63Y3HG"},"source":["def test_image(ind,images_, images_f_2, Model):\n","    image_test = images_f_2[ind]\n","    pred_1 = Model.predict(np.array([image_test]))\n","    sex_f = ['Male', 'Female']\n","    age = int(np.round(pred_1[1][0]))\n","    sex = int(np.round(pred_1[0][0]))\n","    print(\"Predicted Age: \" + str(age))\n","    print(\"Predicted Sex: \" + sex_f[sex])\n","    cv2_imshow( images_[ind])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vhbiGtfY8Ep"},"source":["test_image(57,images_,images_2,Model)\n","\n","print(ages_[57], \" \" , genders_[57] )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rM80D4shZBDR"},"source":["test_image(137,images_,images_2,Model)\n","\n","print(ages_[137], \" \" , genders_[137] )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3Gu8zTEZD3L"},"source":["test_image(502,images_,images_2,Model)\n","\n","print(ages_[502], \" \" , genders_[502] )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIMWScT9_KFf"},"source":["#Model real test con elementos seleccionados aleatoriamente\n","\n","\n","import os\n","fldr = \"drive/MyDrive/Colab Notebooks/Proyecto IA/testRandom\"\n","filesTest=os.listdir(fldr)\n","\n","images_original_size = []\n","imagesTest_f=[]\n","\n","#Extraemos la informacion del dataset\n","for photo in filesTest:\n","  img_dir=fldr+'/'+photo #url de la imagen\n"," \n","  image=cv2.imread(img_dir)\n","\n","  images_original_size.append(image)#para ver el tamanno original\n","\n","  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #simplificamos los tonos de colos\n","  image= cv2.resize(image,(48,48))               #estandarizamos el tamanno de la imagen\n","  imagesTest_f.append(image)\n","\n","temp_imges_test=np.array(imagesTest_f)\n","imagesTest_f_2=temp_imges_test/255 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nnf-5Q-W_S4v"},"source":["valorTest = 6\n","cv2_imshow(images_original_size[valorTest])\n","test_image(valorTest,imagesTest_f,imagesTest_f_2,Model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_xaHkSh-_VJh"},"source":["valorTest = 1\n","cv2_imshow(images_original_size[valorTest])\n","test_image(valorTest,imagesTest_f,imagesTest_f_2,Model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rMo2scj_XN9"},"source":["valorTest = 0\n","cv2_imshow(images_original_size[valorTest])\n","test_image(valorTest,imagesTest_f,imagesTest_f_2,Model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6ZP_ZxO_tIH"},"source":["valorTest = 3\n","cv2_imshow(images_original_size[valorTest])\n","test_image(valorTest,imagesTest_f,imagesTest_f_2,Model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_I-75FEs_w6y"},"source":["valorTest = 4\n","cv2_imshow(images_original_size[valorTest])\n","test_image(valorTest,imagesTest_f,imagesTest_f_2,Model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTAdQb05_2yh"},"source":["valorTest = 7\n","cv2_imshow(images_original_size[valorTest])\n","test_image(valorTest,imagesTest_f,imagesTest_f_2,Model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1lXmWEL_5jr"},"source":["valorTest = 8\n","cv2_imshow(images_original_size[valorTest])\n","test_image(valorTest,imagesTest_f,imagesTest_f_2,Model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDS5Wv54BIix"},"source":["#cargo el modelo entrenado\n","from keras.models import load_model\n","model = load_model('drive/MyDrive/Colab Notebooks/Proyecto IA/modelosGenerados/Model.h5')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmsNo9piBdzx"},"source":["img = tf.keras.utils.plot_model(\n","    model, to_file='model.png', show_shapes=False, show_dtype=False,\n","    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",")\n","img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"orrLYYE6s-_s"},"source":["Convertimos el modelos a tensorflow js\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"m7LpXXbktVO9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KW4_F8q4s-N1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjQi9HqfuSpx"},"source":["pip install tensorflowjs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8l_Tq_AzEl5"},"source":["'./drive/MyDrive/Colab Notebooks/'\n","!dir"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ImKu2JJ3UkRu"},"source":["/content/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYOalbUGVJ95"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/Proyecto IA/modeloFinal"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNcZA5wuVsVJ"},"source":["'./Model2.h5'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uN5GNq5hU5LL"},"source":["\n","!tensorflowjs_converter --input_format=keras ./Model2.h5 ./Model2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xm54os97tMnD"},"source":["const tf = require('@tensorflow/tfjs-node');\n","\n","const ResNet50URI = `file:///${__dirname}/ResNet50/model.json`\n","const ResNet50 = await tf.loadLayersModel(ResNet50URI);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPF-jPdOtS76"},"source":[""],"execution_count":null,"outputs":[]}]}